import logging
import os
import json
import pprint


from parallelm.mcenter_objects import mcenter_defs
from parallelm.mcenter_objects.mcenter_exceptions import InitializationException
from parallelm.mcenter_objects.pipeline_profile import PipelineProfile
from parallelm.mcenter_objects.pipeline_pattern import PipelinePattern
from parallelm.mcenter_objects.model_access import ModelAccess


class Workflow:
    def __init__(self, workflow_data, mlapp_dir, mclient, accounts={}):
        """
        Handles one workflow lifecycle
        """

        self._logger = logging.getLogger(self.__class__.__name__)
        self._workflow_data = workflow_data
        self._mlapp_dir = mlapp_dir
        self._mclient = mclient

        self._logger.info("workflow_data: {}".format(pprint.pformat(workflow_data)))

        self.name = self._workflow_data['name']
        self.sandbox = self._workflow_data.get('sandbox', False)
        self.account_map = accounts
        self.annotation = "Some annotation here"

        annotation = workflow_data.get('annotation')
        if annotation:
            self.annotation = self._workflow_data['annotation']

        self.service_account = None
        # TODO: self._initial_service_account is introduced to keep LDAP mode happy,
        # when internal service account is not in the system
        self._initial_service_account = self._mclient

        # TODO: remove completely the service account in MLApp initialization - only when running
        # self.set_service_account()

        self._workflow_template_file = os.path.expanduser(mcenter_defs.MLAPP_TEMPLATE_FILE)
        if not os.path.isabs(self._workflow_template_file):
            self._workflow_template_file = os.path.abspath(os.path.join(self._mlapp_dir, self._workflow_template_file))

        self._logger.info("workflow_file: {}".format(self._workflow_template_file))

        self._components = self._workflow_data.get('component_paths', [])
        self.upload_components()

        self.pipeline_patterns = {}  # groupID:Pipeline map
        self.pipeline_profiles = {}  # groupID:Pipeline map
        self.groups = {}  # groupID:Group map
        self._workflow_template_data = {}  # loaded workflow .json template
        self.ion_profile_data = {} # ion profile

        self.profile_id = None  # ion profile id
        self.pattern_id = None  # ion pattern id
        self.workflowRunID = None
        self.should_fail = False  # for negative tests
        self._stopped = False
        self._model_ready = False

        # TODO: not supporting models for now
        self._model_access = ModelAccess(mclient, self._mlapp_dir, None)

        self._set_workflow_data()

        self.models = {}       # models generated by run
        self.snapshots = {}
        self.flink_jobs = []
        self.spark_jobs = []

    def check_data(self):
        """
        TODO: move to component
        that will be checking
        integrity of test description
        """
        for pipeline in self.pipeline_patterns.values():
            if pipeline.engine_type == 'spark':
                if self.setup_info.spark_data():
                    raise Exception(
                        "Spark section in test description is needed for running spark pipeline")

    def _set_workflow_data(self):
        """
        Parse workflow file,
        Store info to
        as class members.
        Create workflow in ECO
        """
        self._workflow_template_data = self.load_data(self._workflow_template_file)
        self._workflow_template_data["name"] = self.name
        self._logger.info("w_data: {}".format(pprint.pformat(self._workflow_template_data)))

        # Do this first to make sure all ECO objects
        self.create_pipelines()
        # delete list
        id_list = []
        index = 0

        for section in self._workflow_template_data.get('workflow'):
            s_id = section['id']
            delete_pipeline_id = False
            for key in section.keys():
                if key == "groupId":
                    g_id = self.groups.get(s_id).id
                    section[key] = g_id
                if key in ["pipelineId", "pipelinePatternId"]:
                    section[key] = self.pipeline_patterns[s_id].id
                    # old format, so we will have to delete this later
                    if key == "pipelineId":
                        self._logger.warning('Detected {} key in pipeline description {}, '
                                             'this is old format'.format(key, section))
                        delete_pipeline_id = True

            if delete_pipeline_id is True:
                id_list.append(index)

            # Set cron schedule if specified in TD file
            td_schedule = self.get_pipeline_schedule(s_id)
            self._logger.info('Pipeline schedule to be set to {}'.format(td_schedule))
            if td_schedule:
                section["cronSchedule"] = td_schedule
            index = index + 1

        for idx in id_list:
            self._workflow_template_data.get('workflow')[idx]["pipelinePatternId"] = \
                self._workflow_template_data.get('workflow')[idx].pop("pipelineId")

        self._logger.info('Workflow {} pattern to be created with data {}'.format(self.name, self._workflow_template_data))
        self.pattern_id = self._mclient.create_ion_pattern(self._workflow_template_data)
        if not self.pattern_id:
            raise InitializationException('Failed to create workflow {}'.format(self.name))
        self._logger.info(self.name + ' ID is: ' +
                          json.dumps(self.pattern_id))

        # 1. copy w_data, adjust name for profile
        self.ion_profile_data = self._workflow_template_data.copy()
        self.ion_profile_data["isProfile"] = True
        self.ion_profile_data["ionPatternId"] = self.pattern_id
        self.ion_profile_data["name"] = self.ion_profile_data["name"]
        self.ion_profile_data['serviceAccount'] = self._initial_service_account.user

        # 2. add pipelineAgentSet for each agent in the group
        for section in self.ion_profile_data.get('workflow'):
            s_id = section['id']
            agent_set = []
            for agent_id in self.pipeline_profiles[s_id]:
                agent_set.append({"agentId": agent_id,
                                  "pipelineProfileId": self.pipeline_profiles[s_id][agent_id].id})
            section['pipelineAgentSet'] = agent_set

        self._logger.info("ion profile {}".format(self.ion_profile_data))

        # 3. call ion profile
        self.profile_id = self._mclient.create_ion_profile(self.ion_profile_data)
        if not self.profile_id:
            raise Exception('Could not create profile for {} by '
                            'user {}'.format(self.name, self.service_account['username']))
        self._logger.info("ion profile id {} pattern id {}".format(
            self.profile_id, self.pattern_id))

    def upload_components(self):
        res = True
        for component in self._components:
            if not os.path.isabs(component):
                component = os.path.abspath(os.path.join(self.owl_config_dir_path, component))
            if not os.path.isfile(component):
                self._logger.error(component +
                                   ', needed for ' + self.name +
                                   ' not found')
            else:
                self._logger.info("Uploading component from path: " + component)
                res = self._mclient.upload_component(component)
                if res is None:
                    self._logger.error("Failed to upload " + component)
                    res = False
        return res

    def load_data(self, file_path):
        """
        Reads workflow description
        :param file_path:
        :return: json
        """
        w_data = {}
        if not os.path.exists(file_path):
            raise Exception("Could not find file " + file_path)
        else:
            with open(self._workflow_template_file) as f:
                try:
                    w_data = json.load(f)
                except ValueError as e:
                    self._logger.error("Invalid JSON : %s %s " % (f, e))
                    raise Exception("Invalid JSON : %s %s " % (f, e))

        return w_data

    def _fix_path(self, path):
        exp_path = os.path.expanduser(path)
        return os.path.abspath(os.path.join(self._mlapp_dir, exp_path))

    def create_pipelines(self):
        """
        Creates Pipelines using template data
        from both workflow description of
        pipeline and the pipeline json file
        """
        pipelines_description = self._workflow_data.get('pipelines')
        for p_info in pipelines_description:
            self._logger.info("p_info: {}".format(pprint.pformat(p_info)))
            workflow_ref_id = p_info.get('workflow_ref_id')
            pipeline_mode = self.get_pipeline_mode(workflow_ref_id)
            pipeline_type = self.get_pipeline_type(workflow_ref_id)
            pipeline_id = self.get_pipeline_pattern_id(workflow_ref_id)

            # Extract comparator parameters if exists
            node_a = self.get_node_a(workflow_ref_id)
            agent_name_a = self.get_agent_name(node_a)
            node_b = self.get_node_b(workflow_ref_id)
            agent_name_b = self.get_agent_name(node_b)

            if 'pipeline_pattern' in p_info:
                # new format
                p_info['pipeline_pattern'] = self._fix_path(p_info.get('pipeline_pattern', ''))
                # fix other paths
                profiles_list = p_info.get('profiles')
                if not profiles_list:
                    raise InitializationException('Pipeline descriptions must have "profiles" entry')
                for profile in profiles_list:
                    profile['profile_path'] = self._fix_path(profile.get('profile_path', ''))
            else:
                # old format
                raise InitializationException('Pipeline description '
                                              'must have "pipeline_pattern" entry: {}'
                                              'with pipeline pattern template path'.format(p_info))

            new_pattern = PipelinePattern(p_info,
                                          pipeline_mode,
                                          pipeline_type,
                                          pipeline_id,
                                          node_a, agent_name_a,
                                          node_b, agent_name_b,
                                          self._mlapp_dir,
                                          self._model_access,
                                          self._mclient)
            new_pattern.create()
            self.pipeline_patterns[workflow_ref_id] = new_pattern

            self.groups[workflow_ref_id] = self._all_groups[p_info.get('group')]
            self._logger.info("pattern id {}".format(self.pipeline_patterns[workflow_ref_id].id))

            self.pipeline_profiles[workflow_ref_id] = {}
            for profile in p_info['profiles']:
                self._logger.info("profile: {}".format(pprint.pformat(profile)))
                agent_list = []
                if 'agent' in profile:
                    raise Exception("MLApp definition does not support providing an agent list for now")
                    #agent_list = [self.get_agent_object(self.setup_info.instance_map[x]) for x in profile['agent']]
                elif 'group' in profile:
                    agent_list = self._all_groups[profile['group']].group_agent_objects

                for agent in agent_list:
                    self._logger.info("Agent: {}".format(agent))
                    p_info['profile_path'] = profile['profile_path']
                    # verify if the agents is part of the group

                    new_profile = PipelineProfile(
                        p_info,
                        pipeline_mode,
                        pipeline_type,
                        pipeline_id,
                        node_a, agent_name_a,
                        node_b, agent_name_b,
                        self._mlapp_dir,
                        self._model_access,
                        agent.name,
                        self.pipeline_patterns[workflow_ref_id].id,
                        self._mclient)

                    new_profile.create()
                    self.pipeline_profiles[workflow_ref_id][agent.id] = new_profile
                    self._logger.info("Pipeline Profile id {} -> Pattern id {}".format(
                        self.pipeline_profiles[workflow_ref_id][agent.id].id,
                        self.pipeline_patterns[workflow_ref_id].id))
                    self.profile_vs_pipeline_args(new_profile.id, new_pattern.id)
                if 'profile_path' in p_info:
                    self._logger.info('Popping {}'.format(p_info['profile_path']))
                    p_info.pop('profile_path')

    def profile_vs_pipeline_args(self, profile_id, pattern_id):
        """
        Verification of pipeline arguments
        against pipeline profile arguments.
        The number of arguments should be the same
        Currently there is a problem with default arguments
        :param profile_id:
        :param pattern_id:
        :return:
        """
        all_right = True
        if pattern_id in mcenter_defs.PREDEFINED_PIPELINES + [mcenter_defs.INTERNAL_PREDEFINED_PIPELINE]:
            return all_right
        profile_info = self._mclient.get_pipeline_profile(profile_id)
        pattern_info = self._mclient.get_pipeline_pattern(pattern_id)

        if not profile_info:
            self._logger.error('Could not query pipeline profile info by id: {}'.format(profile_id))
        elif not pattern_info:
            self._logger.error('Could not query pipeline profile info by id: {}'.format(profile_id))
        else:
            profile_args = json.loads(profile_info['pipeline'])
            pattern_args = json.loads(pattern_info['pipeline'])

            for section in profile_args['pipe']:

                pattern_corresponding_section = [x for x in pattern_args['pipe']
                                                 if x['id'] == section['id']]
                if not pattern_corresponding_section:
                    self._logger.error('Could not find section {} from the profile in '
                                       'corresponding pattern'.format(section['name']))
                    all_right = False
                else:
                    pattern_corresponding_section = pattern_corresponding_section[0]
                    self._logger.info('pattern component "{}"-> profile component "{}" '
                                      'arguments in pipeline pattern->profile: '
                                      '{} -> {}'.format(section['name'], pattern_corresponding_section['name'],
                                                        pattern_corresponding_section['arguments'],
                                                        section['arguments']))
                    if not len(profile_args) == len(pattern_args):
                        self._logger.warning('Pipeline pattern and profile argument numbers do not correspond')

        return all_right

    def set_service_account(self):
        """
        This method sets the workflow owner and launcher users.
        If "workflow" description nas "name" key,
        then the value of that key will be set as workflow owner.
        Otherwise, "admin" will be the workflow owner.
        Likewise, the workflow launcher will be the "name" value, or the default "mcenter",
        since MCenter's default "admin" has no permission to launch workflow in production.
        TODO: with RBAC support, this implementation will be more flexible
        :return:
        """
        # Get "name" from workflow
        username = self._workflow_data.get('username')
        # Initialize to "mcenter"
        service_account_username = mcenter_defs.DEFAULT_SERVICE_USERNAME
        # If no username is requested to own the workflow creation

        if username:
            if username not in self.account_map.keys():
                # "mcenter" is created by owl always in case no other user is specified
                raise InitializationException('User {} requested in "workflow" section {} '
                                              'does not exist'.format(username, self.name))
            self._mclient = self.account_map[username]
            self._initial_service_account = self.account_map[username]
            service_account_username = username

        # Double check user exists and not None
        if not self._mclient:
            raise InitializationException('User account {} should exist'.format(username))

        # Find the requested user in MCenter
        user_data = self.find_user(service_account_username)
        if not user_data:
            raise Exception('Could not query the user {} in MCenter'.format(service_account_username))
        self.service_account = user_data
        self._logger.info('Got the service account "{}" for this MLApp {}'.format(username, self.name))
        self._logger.info('All other operations for now will be ran by "{}" for this MLApp {}'.format(self._mclient.user, self.name))

    def launch(self):
        """
        Launches workflow
        """
        self._logger.info('Instances already running: {}'.format(self._mclient.list_workflow_instances()))

        self.workflowRunID = self._mclient.launch_workflow(self.profile_id,
                                                           self.service_account['username'],
                                                           annotation=self.annotation,
                                                           sandbox=self.sandbox)

        self._logger.info('Launched workflow: ' + self.name +
                          ' with runid: ' + str(self.workflowRunID) +
                          ' annotation: ' + str(self.annotation) +
                          ' sandbox mode: ' + str(self.sandbox))
        return self.workflowRunID

    def set_model(self, event_data):
        """
        Set model (rollback) for running ION
        TODO: Sometimes user may want to set more than one pipeline
        with same model. Currently this function handles setting
        one pipeline, though the workflow node ids is a list in
        test_description file.
        TODO: support the above
        TODO: add check that the set model was used actually
        :param model_index:   reference to index based all models per ION
        :param model_name:    Model name optional
        :return:
        """
        if len(event_data.workflow_node_ids) > 1:
            self._logger.info('Note: currently model can be set for one node at a time')
        w_details = self._mclient.running_workflow_details(self.workflowRunID)
        pipeline_map = w_details['pipelineInstanceIdToWfNodeId']
        for p_id in pipeline_map.keys():
            if pipeline_map[p_id] in event_data.workflow_node_ids:
                pipeline_instance_id = p_id
        models = self.get_all_models()

        model = {}
        if event_data.model_name is not None:
            model = models.get(event_data.model_name, None)

        elif event_data.model_index < len(models):
            model = list(models.values())[event_data.model_index]
        result = ""
        if model is not None:
            self._logger.info('Rolling back Model ' + model.get('id') +
                              ' with run id: ' + str(self.workflowRunID) +
                              ' for pipeline with id ' + pipeline_instance_id)
            result = self._mclient.set_model(model.get('id'), self.workflowRunID,
                                             pipeline_instance_id)
            self._logger.info('Model setting result {}'.format(result))
        else:
            self._logger.error('Model by name {} requested for setting in workflow {} '
                               'but could not be found in '
                               'database'.format(event_data.model_name, self.workflowRunID))
        return result

    def create_snapshot(self, snapshot_name):
        """
        As snapshot is created
        per running ION,
        we delegate snapshot creation
        to Workflow class
        """
        self._logger.info('Creating snapshot for workflow ' +
                          self.name + ' with id ' + self.workflowRunID)
        snapshot = Snapshot(self.setup_info, self.workflowRunID, snapshot_name)
        snapshot.create()
        self.snapshots[snapshot_name] = snapshot
        return snapshot

    def move_to_production(self):
        """
        """
        self._logger.info('Moving ION: ' + self.name + ' with run id: ' + str(self.workflowRunID) + ' to production')
        result = self._mclient.move_to_production(self.workflowRunID, self.service_account['username'], self.annotation)
        return result

    def stop(self):
        """ Stops one workflow """
        self._logger.info("Calling api to stop workflow {} with id {}".format(self.name, self.workflowRunID))
        result = self._mclient.stop_workflow(self.workflowRunID)
        if result == {}:
            self._logger.info('Workflow successfully stopped')
            self._stopped = True
            return True
        self._logger.error('Failed to stop workflow')
        return False

    @property
    def launch_successful(self):
        if self.workfloRunID:
            return True
        return False

    @property
    def stop_successful(self):
        if self._stopped:
            return True
        return False

    @property
    def running(self):
        details = self.running_workflow_details()
        if details:
            status = details.get('status')
            if status == 'RUNNING':
                return True
        return False

    @property
    def model_ready(self):
        return self._model_ready

    @property
    def completed(self):
        details = self._mclient.ion_pattern_details(self.workflowRunID)
        if details:
            status = details.get('status')
            if status == 'COMPLETED':
                return True
        return False

    @property
    def failed(self):
        """ TODO: this will work in future only """
        details = self.workflow_details()
        if details:
            status = details.get('status')
            if status == 'FAILED':
                return True
        return False

    def workflow_databases(self):
        return self.setup_info.sql_db_database

    # HELPERS
    def get_node_a(self, workflow_ref_id):
        """
        Helper to get the workflowNodeIdA
        from workflow template
        Return None if not found.
        Workflow_ref_id is the reference
        of the pipeline in workflow
        json hierarchy
        """
        if "comparisonWorkflowNodeSets" in self._workflow_template_data:
            comparison_list = self._workflow_template_data['comparisonWorkflowNodeSets']
            for comparison_set in comparison_list:
                if comparison_set['workflowNodeIdComparator'] == workflow_ref_id:
                    return comparison_set['workflowNodeIdA']

    def get_node_b(self, workflow_ref_id):
        """
        Helper to get the workflowNodeIdB
        from workflow template
        Return None if not found.
        Workflow_ref_id is the reference
        of the pipeline in workflow
        json hierarchy
        """
        if "comparisonWorkflowNodeSets" in self._workflow_template_data:
            comparison_list = self._workflow_template_data['comparisonWorkflowNodeSets']
            for comparison_set in comparison_list:
                if comparison_set['workflowNodeIdComparator'] == workflow_ref_id:
                    return comparison_set['workflowNodeIdB']

    def get_agent_name(self, workflow_ref_id):
        """
        Helper to get the agent name
        from test description
        Return None if not found.
        Workflow_ref_id is the reference
        of the pipeline in workflow
        json hierarchy
        """
        pipelines_description = self._workflow_data.get('pipelines')

        for p_info in pipelines_description:
            if p_info.get('workflow_ref_id') == workflow_ref_id:
                group = p_info.get('group')
                return self._all_groups[group].agent_names[0]

    def get_pipeline_mode(self, workflow_ref_id):
        """
        Helper to get the pipelineMode
        from workflow template
        Return None if not found.
        Workflow_ref_id is the reference
        of the pipeline in workflow
        json hierarchy
        """
        workflow_list = self._workflow_template_data['workflow']
        for workflow_info in workflow_list:
            if workflow_info['id'] == workflow_ref_id:
                return workflow_info['pipelineMode']

    def get_pipeline_type(self, workflow_ref_id):
        """
        Helper to get the pipelineMode
        from workflow template.
        Workflow_ref_id is the reference
        of the pipeline in workflow
        json hierarchy
        """
        workflow_list = self._workflow_template_data['workflow']
        for workflow_info in workflow_list:
            if workflow_info['id'] == workflow_ref_id:
                return workflow_info['pipelineType']

    def get_pipeline_schedule(self, workflow_ref_id):
        """
        Helper to get the cron schedule
        from test description file.
        Workflow_ref_id is the reference
        of the pipeline in workflow
        json hierarchy
        """
        for pipeline_info in self._workflow_data.get('pipelines', []):
            if pipeline_info["workflow_ref_id"] == workflow_ref_id:
                schedule = pipeline_info.get("cronSchedule", None)
                return schedule

    def get_pipeline_profile_id(self, workflow_ref_id):
        """
        Helper to get the list of profile ids for a workflow node
        :param workflow_ref_id: ref id in the workflow section of the owl template
        :return: hash map agent_id -> profile_id
        """
        return self.pipeline_profiles[workflow_ref_id]

    def get_pipeline_pattern_id(self, workflow_ref_id):
        """
        Helper to get the pipelineMode
        from workflow template.
        Workflow_ref_id is the reference
        of the pipeline in workflow
        json hierarchy
        """
        workflow_list = self._workflow_template_data['workflow']
        for workflow_info in workflow_list:
            if workflow_info['id'] == workflow_ref_id:
                if 'pipelineId' in workflow_info:
                    return workflow_info['pipelineId']
                elif 'pipelinePatternId' in workflow_info:
                    return workflow_info['pipelinePatternId']
                else:
                    raise Exception("Invalid workflow section - cant find pipelineId or "
                                    "pipelinePatternId")

    def print_self(self):
        """ Prints the Workflow class info """
        self._logger.info(self.profile_id)

    def dump(self, workflow):
        """ Saves to file """
        dumps = json.dumps(workflow, indent=4)
        with open(self.name + '_workflow_dump.json', 'w') as f:
            f.write(dumps)

    def list_workflows(self):
        """ List all workflows, TODO: move up """
        w = self._mclient.list_ion_patterns()
        self._logger.info(len(w.get('workflows', {})))

    def group_names(self):
        """
        Returns group
        names involved in
        workflow run
        """
        group_names = []
        for group in self.groups.keys():
            group_names.append(self.groups[group].name)
        return group_names

    @property
    def agent_names(self):
        """
        Returns the agent names
        involved in workflow
        """
        agents = []
        for group_name in self.group_names():
            agents += self._all_groups[group_name].agent_names
        return list(set(agents))

    @property
    def agent_ids(self):
        """
        Query REST api
        :returns: list
        """
        agent_ids = self._mclient.get_workflow_agent_ids(self.profile_id)
        return agent_ids

    @property
    def group_ids(self):
        w = self._mclient.ion_pattern_details(self.profile_id)
        groups = [x["groupId"] for x in w]
        return groups

    @property
    def agents(self):
        """
        :return: list of Agents
        Utility to get all
        Agents for the Workflow
        """
        agents = []
        for group in self.groups.values():
            agents.extend(group.agents)
        return agents

    @property
    def pm_agents(self):
        """
        :return: list of
        Flink running Agents
        Utility to get all
        Agents for the Workflow
        """
        objects = []
        for section in self.workflow_details().get('workflow'):
            if section['pipelineMode'] == 'online':
                group_details = self._mclient.get_group(section['groupId'])
                agents = group_details['agents']
                for agent_id in agents:
                    name_string = self._mclient.get_agent(agent_id)['address']
                    agent_object = self.get_agent_object(name_string)
                    objects.append(agent_object)
        return objects

    def get_agent_object(self, agent_name):
        """
        :return: Agent
        Query and return Agent
        object by its name
        """
        for name in self.group_names():
            group = self._all_groups[name]
            for agent in group.group_agent_objects:
                if agent.name == agent_name:
                    return agent

    def agents_status(self):
        """
        Tuple: (alive agent names, dead agent names)
        """
        alive = []
        dead = []
        return (alive, dead)

    def workflow_details(self):
        details = self._mclient.ion_pattern_details(self.profile_id)
        if not details:
            self._logger.error('Could not query workflow {} details'.format(self.profile_id))
        return details

    def running_workflow_details(self):
        details = self._mclient.running_workflow_details(self.workflowRunID)
        return details

    def get_all_models(self):
        """
        Get all models
        """
        all_models = self._mclient.get_models(timeout=30)
        models_by_name = {}
        if all_models is not None:
            for m in all_models:
                name = m.get('name')
                models_by_name[name] = m
        return models_by_name

    def get_models(self, workflowRunID):
        """
        Get workflow specific models
        """
        all_models = self._mclient.get_models(timeout=30)
        if all_models is not None:
            for m in all_models:
                if m.get('workflowRunId') == workflowRunID:
                    name = m.get('name')
                    self.models[name] = m
        if self.models:
            self._model_ready = True
        return self.models

    def expect_model(self):
        """
        Depending on pipeline types
        model generation is different.
        Check if there is model_producer
        in involved pipelines
        """
        details = self.workflow_details()
        if not details:
            return False
        pipeline_types = [x['pipelineType'] for x in details['workflow']]
        if MODEL_PRODUCER in pipeline_types:
            return True
        if MODEL_PRODUCER_CONSUMER in pipeline_types:
            return True
        else:
            self._logger.info('Workflow {} does not have \
                              model_producing pipelines, \
                              no model generation is expected'.format(self.profile_id))
        return False

    def get_pipeline_info(self, workflow_ref_id):
        workflow_info = self._mclient.running_workflow_details(self.workflowRunID)
        if not workflow_info:
            return []
        return workflow_info.get('pipelineInstances', [])

    def pipeline_database(self, workflow_ref_id):
        """
        Returns the database names for a
        specific workflow ref id
        """
        workflow_info = self._mclient.running_workflow_details(self.workflowRunID)
        if not workflow_info:
            return []
        pipeline_info = workflow_info.get('pipelineInstances', [])
        pipeline_db_names = [x['pipelineSystemParams']['statsMeasurementID'] for x in pipeline_info if x['workflowNodeId'] == workflow_ref_id]

        return pipeline_db_names

    def planned_snapshots(self):
        planned = []

        timeline = self.setup_info.timeline
        for event in timeline:
            if event['event']['name'] == 'create_snapshot':
                if event['event']['workflow'] == self.name:
                    planned.append(event['event']['snapshot_name'])
        return planned

    def existing_snapshots(self):
        snapshots = self._mclient.list_snapshots()
        existing = [x['name'] for x in snapshots if x['workflowInstance']['workflow']['id'] == self.profile_id]
        self._logger.info(self.name + ' existing snapshots: ' + str(existing))
        return existing

    def planned_snapshot_exports(self):
        """
        Get all the snapshot paths
        to be exported for workflow
        """
        planned = []

        timeline = self.setup_info.timeline
        for event in timeline:
            if event['event']['name'] == 'export_snapshot':
                if event['event']['workflow'] == self.name:
                    planned.append(event['event']['file_path'])
        self._logger.info(self.name + ' to export these snapshots to paths: ' + str(planned))
        return planned

    def get_pipeline_instance_id(self, w_data, test_data):
        """
        Helper to get information about pipeline
        given test description section for pipeline
        and ION instance data
        :param w_data:
        :param test_data:
        :return: pipeline instance id
        """
        pipeline_instance_id = None
        workflow_ref_id = test_data["workflow_ref_id"]
        reflex_instance = test_data["reflex_instance"]

        hostname = self.setup_info._json_data['physical']["reflex_instances"][reflex_instance][0]

        for pipeline in w_data['pipelineInstances']:
            if str(pipeline['workflowNodeId']) == str(workflow_ref_id) \
                    and pipeline['hostname'] == hostname:
                pipeline_instance_id = pipeline['pipelineInstanceId']

        return pipeline_instance_id

    def find_user(self, username):
        existing_users = self._mclient.get_users()
        service_user = [x for x in existing_users if x['username'] == username]
        if service_user:
            return service_user[0]
